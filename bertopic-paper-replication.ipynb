{"cells":[{"cell_type":"markdown","metadata":{},"source":["Github repo for the original paper: [https://github.com/MaartenGr/BERTopic_evaluation/tree/main](https://github.com/MaartenGr/BERTopic_evaluation/tree/main)\n","\n","* All evaluations are carried out in Kaggle notebook thus were contrained  by the Kaggle's 12-hour training limit, thus we could not finish running LDA sequence model. Similarly, CTM evaluations for 20NewsGroup and BBC News datasets are also skipped.\n","* We skipped all models using `Top2Vec` or `Doc2Vec` models due to unsolved conflicts caused by the older version of the `gensim` library.\n","\n","* We did not test Wall Time of models as packages/libraries have changed significantly in the last two years, so the results won't be comparable with the original paper."]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-07-27T18:01:45.346682Z","iopub.status.busy":"2024-07-27T18:01:45.345507Z","iopub.status.idle":"2024-07-27T18:02:00.275568Z","shell.execute_reply":"2024-07-27T18:02:00.273785Z","shell.execute_reply.started":"2024-07-27T18:01:45.346618Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: cython<3.0.0 in /opt/conda/lib/python3.10/site-packages (0.29.37)\n"]}],"source":["!pip install \"cython<3.0.0\""]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-07-27T18:02:00.278277Z","iopub.status.busy":"2024-07-27T18:02:00.277734Z","iopub.status.idle":"2024-07-27T18:02:27.271193Z","shell.execute_reply":"2024-07-27T18:02:27.269625Z","shell.execute_reply.started":"2024-07-27T18:02:00.278226Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting pyyaml==5.4.1\n","  Downloading PyYAML-5.4.1.tar.gz (175 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.1/175.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n","\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25hBuilding wheels for collected packages: pyyaml\n","  Building wheel for pyyaml (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for pyyaml: filename=PyYAML-5.4.1-cp310-cp310-linux_x86_64.whl size=155376 sha256=788c7f8c27cdec6c7cd5b13f253ce9520cdc0b976385c2cd864236be1607131d\n","  Stored in directory: /root/.cache/pip/wheels/c7/0d/22/696ee92245ad710f506eee79bb05c740d8abccd3ecdb778683\n","Successfully built pyyaml\n","Installing collected packages: pyyaml\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 6.0.1\n","    Uninstalling PyYAML-6.0.1:\n","      Successfully uninstalled PyYAML-6.0.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","jupyterlab 4.2.3 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\n","jupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\n","kfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\n","ydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed pyyaml-5.4.1\n"]}],"source":["!pip install --no-build-isolation pyyaml==5.4.1  # Try installing pyyaml again"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-07-27T18:02:27.276015Z","iopub.status.busy":"2024-07-27T18:02:27.275404Z","iopub.status.idle":"2024-07-27T18:03:52.962070Z","shell.execute_reply":"2024-07-27T18:03:52.960429Z","shell.execute_reply.started":"2024-07-27T18:02:27.275959Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\n","tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.4.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mCollecting contextualized_topic_models==2.2.1\n","  Downloading contextualized_topic_models-2.2.1-py2.py3-none-any.whl.metadata (23 kB)\n","Requirement already satisfied: numpy>=1.19.1 in /opt/conda/lib/python3.10/site-packages (from contextualized_topic_models==2.2.1) (1.26.4)\n","Requirement already satisfied: torchvision>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from contextualized_topic_models==2.2.1) (0.16.2+cpu)\n","Requirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from contextualized_topic_models==2.2.1) (2.1.2+cpu)\n","Requirement already satisfied: gensim>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from contextualized_topic_models==2.2.1) (4.3.2)\n","Collecting sentence-transformers>=1.1.1 (from contextualized_topic_models==2.2.1)\n","  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: wordcloud>=1.8.1 in /opt/conda/lib/python3.10/site-packages (from contextualized_topic_models==2.2.1) (1.9.3)\n","Requirement already satisfied: matplotlib>=3.1.3 in /opt/conda/lib/python3.10/site-packages (from contextualized_topic_models==2.2.1) (3.7.5)\n","Requirement already satisfied: tqdm>=4.56.0 in /opt/conda/lib/python3.10/site-packages (from contextualized_topic_models==2.2.1) (4.66.4)\n","Requirement already satisfied: scipy>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from contextualized_topic_models==2.2.1) (1.11.4)\n","Collecting ipywidgets==7.5.1 (from contextualized_topic_models==2.2.1)\n","  Downloading ipywidgets-7.5.1-py2.py3-none-any.whl.metadata (1.8 kB)\n","Collecting ipython==7.16.1 (from contextualized_topic_models==2.2.1)\n","  Downloading ipython-7.16.1-py3-none-any.whl.metadata (4.4 kB)\n","Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.10/site-packages (from ipython==7.16.1->contextualized_topic_models==2.2.1) (69.0.3)\n","Requirement already satisfied: jedi>=0.10 in /opt/conda/lib/python3.10/site-packages (from ipython==7.16.1->contextualized_topic_models==2.2.1) (0.19.1)\n","Requirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ipython==7.16.1->contextualized_topic_models==2.2.1) (5.1.1)\n","Requirement already satisfied: pickleshare in /opt/conda/lib/python3.10/site-packages (from ipython==7.16.1->contextualized_topic_models==2.2.1) (0.7.5)\n","Requirement already satisfied: traitlets>=4.2 in /opt/conda/lib/python3.10/site-packages (from ipython==7.16.1->contextualized_topic_models==2.2.1) (5.9.0)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from ipython==7.16.1->contextualized_topic_models==2.2.1) (3.0.42)\n","Requirement already satisfied: pygments in /opt/conda/lib/python3.10/site-packages (from ipython==7.16.1->contextualized_topic_models==2.2.1) (2.17.2)\n","Collecting backcall (from ipython==7.16.1->contextualized_topic_models==2.2.1)\n","  Downloading backcall-0.2.0-py2.py3-none-any.whl.metadata (2.0 kB)\n","Requirement already satisfied: pexpect in /opt/conda/lib/python3.10/site-packages (from ipython==7.16.1->contextualized_topic_models==2.2.1) (4.8.0)\n","Requirement already satisfied: ipykernel>=4.5.1 in /opt/conda/lib/python3.10/site-packages (from ipywidgets==7.5.1->contextualized_topic_models==2.2.1) (6.28.0)\n","Requirement already satisfied: nbformat>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets==7.5.1->contextualized_topic_models==2.2.1) (5.9.2)\n","Collecting widgetsnbextension~=3.5.0 (from ipywidgets==7.5.1->contextualized_topic_models==2.2.1)\n","  Downloading widgetsnbextension-3.5.2-py2.py3-none-any.whl.metadata (1.3 kB)\n","Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.10/site-packages (from gensim>=3.8.3->contextualized_topic_models==2.2.1) (6.4.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.1.3->contextualized_topic_models==2.2.1) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.1.3->contextualized_topic_models==2.2.1) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.1.3->contextualized_topic_models==2.2.1) (4.47.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.1.3->contextualized_topic_models==2.2.1) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.1.3->contextualized_topic_models==2.2.1) (21.3)\n","Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.1.3->contextualized_topic_models==2.2.1) (9.5.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.1.3->contextualized_topic_models==2.2.1) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.1.3->contextualized_topic_models==2.2.1) (2.9.0.post0)\n","Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=1.1.1->contextualized_topic_models==2.2.1) (4.42.3)\n","Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=1.1.1->contextualized_topic_models==2.2.1) (1.2.2)\n","Requirement already satisfied: huggingface-hub>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=1.1.1->contextualized_topic_models==2.2.1) (0.23.4)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->contextualized_topic_models==2.2.1) (3.13.1)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->contextualized_topic_models==2.2.1) (4.9.0)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->contextualized_topic_models==2.2.1) (1.13.0)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->contextualized_topic_models==2.2.1) (3.2.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->contextualized_topic_models==2.2.1) (3.1.2)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->contextualized_topic_models==2.2.1) (2024.5.0)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision>=0.7.0->contextualized_topic_models==2.2.1) (2.32.3)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers>=1.1.1->contextualized_topic_models==2.2.1) (5.4.1)\n","Requirement already satisfied: comm>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets==7.5.1->contextualized_topic_models==2.2.1) (0.2.1)\n","Requirement already satisfied: debugpy>=1.6.5 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets==7.5.1->contextualized_topic_models==2.2.1) (1.8.0)\n","INFO: pip is looking at multiple versions of ipykernel to determine which version is compatible with other requirements. This could take a while.\n","Collecting ipykernel>=4.5.1 (from ipywidgets==7.5.1->contextualized_topic_models==2.2.1)\n","  Downloading ipykernel-6.29.5-py3-none-any.whl.metadata (6.3 kB)\n","  Downloading ipykernel-6.29.4-py3-none-any.whl.metadata (6.3 kB)\n","  Downloading ipykernel-6.29.3-py3-none-any.whl.metadata (6.3 kB)\n","  Downloading ipykernel-6.29.2-py3-none-any.whl.metadata (6.0 kB)\n","  Downloading ipykernel-6.29.1-py3-none-any.whl.metadata (6.0 kB)\n","  Downloading ipykernel-6.29.0-py3-none-any.whl.metadata (6.0 kB)\n","  Downloading ipykernel-6.27.1-py3-none-any.whl.metadata (6.3 kB)\n","INFO: pip is still looking at multiple versions of ipykernel to determine which version is compatible with other requirements. This could take a while.\n","  Downloading ipykernel-6.26.0-py3-none-any.whl.metadata (6.3 kB)\n","  Downloading ipykernel-6.25.2-py3-none-any.whl.metadata (6.3 kB)\n","  Downloading ipykernel-6.25.1-py3-none-any.whl.metadata (6.3 kB)\n","  Downloading ipykernel-6.25.0-py3-none-any.whl.metadata (6.3 kB)\n","  Downloading ipykernel-6.24.0-py3-none-any.whl.metadata (6.3 kB)\n","INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n","  Downloading ipykernel-6.23.3-py3-none-any.whl.metadata (6.3 kB)\n","  Downloading ipykernel-6.23.2-py3-none-any.whl.metadata (6.3 kB)\n","  Downloading ipykernel-6.23.1-py3-none-any.whl.metadata (6.3 kB)\n","  Downloading ipykernel-6.23.0-py3-none-any.whl.metadata (6.3 kB)\n","  Downloading ipykernel-6.22.0-py3-none-any.whl.metadata (6.4 kB)\n","  Downloading ipykernel-6.21.3-py3-none-any.whl.metadata (6.4 kB)\n","  Downloading ipykernel-6.21.2-py3-none-any.whl.metadata (6.8 kB)\n","  Downloading ipykernel-6.21.1-py3-none-any.whl.metadata (6.8 kB)\n","  Downloading ipykernel-6.20.2-py3-none-any.whl.metadata (6.7 kB)\n","  Downloading ipykernel-6.20.1-py3-none-any.whl.metadata (6.7 kB)\n","  Downloading ipykernel-6.19.4-py3-none-any.whl.metadata (6.4 kB)\n","  Downloading ipykernel-6.19.3-py3-none-any.whl.metadata (6.4 kB)\n","  Downloading ipykernel-6.19.2-py3-none-any.whl.metadata (6.7 kB)\n","  Downloading ipykernel-6.19.1-py3-none-any.whl.metadata (6.7 kB)\n","  Downloading ipykernel-6.17.1-py3-none-any.whl.metadata (5.6 kB)\n","  Downloading ipykernel-6.17.0-py3-none-any.whl.metadata (5.6 kB)\n","  Downloading ipykernel-6.16.2-py3-none-any.whl.metadata (5.6 kB)\n","  Downloading ipykernel-6.16.1-py3-none-any.whl.metadata (5.6 kB)\n","  Downloading ipykernel-6.16.0-py3-none-any.whl.metadata (5.4 kB)\n","  Downloading ipykernel-6.15.3-py3-none-any.whl.metadata (5.4 kB)\n","  Downloading ipykernel-6.15.2-py3-none-any.whl.metadata (5.4 kB)\n","  Downloading ipykernel-6.15.1-py3-none-any.whl.metadata (5.4 kB)\n","  Downloading ipykernel-6.15.0-py3-none-any.whl.metadata (5.4 kB)\n","  Downloading ipykernel-6.14.0-py3-none-any.whl.metadata (5.4 kB)\n","  Downloading ipykernel-6.13.1-py3-none-any.whl.metadata (5.4 kB)\n","  Downloading ipykernel-6.13.0-py3-none-any.whl.metadata (2.4 kB)\n","  Downloading ipykernel-6.12.1-py3-none-any.whl.metadata (2.2 kB)\n","  Downloading ipykernel-6.12.0-py3-none-any.whl.metadata (2.2 kB)\n","  Downloading ipykernel-6.11.0-py3-none-any.whl.metadata (2.2 kB)\n","  Downloading ipykernel-6.10.0-py3-none-any.whl.metadata (2.0 kB)\n","  Downloading ipykernel-6.9.2-py3-none-any.whl.metadata (2.0 kB)\n","  Downloading ipykernel-6.9.1-py3-none-any.whl.metadata (2.0 kB)\n","  Downloading ipykernel-6.9.0-py3-none-any.whl.metadata (2.0 kB)\n","  Downloading ipykernel-6.8.0-py3-none-any.whl.metadata (2.0 kB)\n","  Downloading ipykernel-6.7.0-py3-none-any.whl.metadata (2.0 kB)\n","  Downloading ipykernel-6.6.1-py3-none-any.whl.metadata (2.1 kB)\n","  Downloading ipykernel-6.6.0-py3-none-any.whl.metadata (2.1 kB)\n","  Downloading ipykernel-6.5.1-py3-none-any.whl.metadata (2.1 kB)\n","  Downloading ipykernel-6.5.0-py3-none-any.whl.metadata (2.1 kB)\n","  Downloading ipykernel-6.4.2-py3-none-any.whl.metadata (2.2 kB)\n","Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets==7.5.1->contextualized_topic_models==2.2.1) (0.2.0)\n","  Downloading ipykernel-6.4.1-py3-none-any.whl.metadata (2.0 kB)\n","  Downloading ipykernel-6.4.0-py3-none-any.whl.metadata (2.0 kB)\n","  Downloading ipykernel-6.3.1-py3-none-any.whl.metadata (2.0 kB)\n","  Downloading ipykernel-6.3.0-py3-none-any.whl.metadata (2.0 kB)\n","  Downloading ipykernel-6.2.0-py3-none-any.whl.metadata (2.1 kB)\n","  Downloading ipykernel-6.1.0-py3-none-any.whl.metadata (2.1 kB)\n","  Downloading ipykernel-6.0.3-py3-none-any.whl.metadata (2.0 kB)\n","  Downloading ipykernel-6.0.2-py3-none-any.whl.metadata (2.0 kB)\n","  Downloading ipykernel-6.0.1-py3-none-any.whl.metadata (2.0 kB)\n","  Downloading ipykernel-6.0.0-py3-none-any.whl.metadata (2.0 kB)\n","  Downloading ipykernel-5.5.6-py3-none-any.whl.metadata (1.1 kB)\n","Requirement already satisfied: jupyter-client in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets==7.5.1->contextualized_topic_models==2.2.1) (7.4.9)\n","Requirement already satisfied: tornado>=4.2 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.5.1->ipywidgets==7.5.1->contextualized_topic_models==2.2.1) (6.3.3)\n","Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.10->ipython==7.16.1->contextualized_topic_models==2.2.1) (0.8.3)\n","Requirement already satisfied: fastjsonschema in /opt/conda/lib/python3.10/site-packages (from nbformat>=4.2.0->ipywidgets==7.5.1->contextualized_topic_models==2.2.1) (2.19.1)\n","Requirement already satisfied: jsonschema>=2.6 in /opt/conda/lib/python3.10/site-packages (from nbformat>=4.2.0->ipywidgets==7.5.1->contextualized_topic_models==2.2.1) (4.20.0)\n","Requirement already satisfied: jupyter-core in /opt/conda/lib/python3.10/site-packages (from nbformat>=4.2.0->ipywidgets==7.5.1->contextualized_topic_models==2.2.1) (5.7.1)\n","Requirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython==7.16.1->contextualized_topic_models==2.2.1) (0.2.13)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.1.3->contextualized_topic_models==2.2.1) (1.16.0)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers>=1.1.1->contextualized_topic_models==2.2.1) (2023.12.25)\n","Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers>=1.1.1->contextualized_topic_models==2.2.1) (0.4.3)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers>=1.1.1->contextualized_topic_models==2.2.1) (0.19.1)\n","Requirement already satisfied: notebook>=4.4.1 in /opt/conda/lib/python3.10/site-packages (from widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models==2.2.1) (6.5.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->contextualized_topic_models==2.2.1) (2.1.3)\n","Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.10/site-packages (from pexpect->ipython==7.16.1->contextualized_topic_models==2.2.1) (0.7.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.7.0->contextualized_topic_models==2.2.1) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.7.0->contextualized_topic_models==2.2.1) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.7.0->contextualized_topic_models==2.2.1) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision>=0.7.0->contextualized_topic_models==2.2.1) (2024.7.4)\n","Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers>=1.1.1->contextualized_topic_models==2.2.1) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers>=1.1.1->contextualized_topic_models==2.2.1) (3.2.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->contextualized_topic_models==2.2.1) (1.3.0)\n","Requirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets==7.5.1->contextualized_topic_models==2.2.1) (23.2.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets==7.5.1->contextualized_topic_models==2.2.1) (2023.12.1)\n","Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets==7.5.1->contextualized_topic_models==2.2.1) (0.32.1)\n","Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets==7.5.1->contextualized_topic_models==2.2.1) (0.16.2)\n","Requirement already satisfied: pyzmq>=17 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models==2.2.1) (24.0.1)\n","Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models==2.2.1) (23.1.0)\n","Requirement already satisfied: nbconvert>=5 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models==2.2.1) (6.4.5)\n","Requirement already satisfied: nest-asyncio>=1.5 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models==2.2.1) (1.5.8)\n","Requirement already satisfied: Send2Trash>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models==2.2.1) (1.8.2)\n","Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models==2.2.1) (0.18.0)\n","Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models==2.2.1) (0.19.0)\n","Requirement already satisfied: nbclassic>=0.4.7 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models==2.2.1) (1.0.0)\n","Requirement already satisfied: entrypoints in /opt/conda/lib/python3.10/site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets==7.5.1->contextualized_topic_models==2.2.1) (0.4)\n","Requirement already satisfied: platformdirs>=2.5 in /opt/conda/lib/python3.10/site-packages (from jupyter-core->nbformat>=4.2.0->ipywidgets==7.5.1->contextualized_topic_models==2.2.1) (3.11.0)\n","Requirement already satisfied: jupyter-server>=1.8 in /opt/conda/lib/python3.10/site-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models==2.2.1) (2.12.5)\n","Requirement already satisfied: notebook-shim>=0.2.3 in /opt/conda/lib/python3.10/site-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models==2.2.1) (0.2.3)\n","Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models==2.2.1) (0.8.4)\n","Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models==2.2.1) (0.3.0)\n","Requirement already satisfied: bleach in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models==2.2.1) (6.1.0)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models==2.2.1) (1.5.0)\n","Requirement already satisfied: testpath in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models==2.2.1) (0.6.0)\n","Requirement already satisfied: defusedxml in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models==2.2.1) (0.7.1)\n","Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models==2.2.1) (4.12.2)\n","Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models==2.2.1) (0.5.13)\n","Requirement already satisfied: argon2-cffi-bindings in /opt/conda/lib/python3.10/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models==2.2.1) (21.2.0)\n","Requirement already satisfied: anyio>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models==2.2.1) (4.2.0)\n","Requirement already satisfied: jupyter-events>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models==2.2.1) (0.9.0)\n","Requirement already satisfied: jupyter-server-terminals in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models==2.2.1) (0.5.1)\n","Requirement already satisfied: overrides in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models==2.2.1) (7.4.0)\n","Requirement already satisfied: websocket-client in /opt/conda/lib/python3.10/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models==2.2.1) (1.7.0)\n","Requirement already satisfied: cffi>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models==2.2.1) (1.16.0)\n","Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models==2.2.1) (2.5)\n","Requirement already satisfied: webencodings in /opt/conda/lib/python3.10/site-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models==2.2.1) (0.5.1)\n","Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models==2.2.1) (1.3.0)\n","Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models==2.2.1) (1.2.0)\n","Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models==2.2.1) (2.21)\n","Requirement already satisfied: python-json-logger>=2.0.4 in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models==2.2.1) (2.0.7)\n","Requirement already satisfied: rfc3339-validator in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models==2.2.1) (0.1.4)\n","Requirement already satisfied: rfc3986-validator>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models==2.2.1) (0.1.1)\n","Requirement already satisfied: fqdn in /opt/conda/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models==2.2.1) (1.5.1)\n","Requirement already satisfied: isoduration in /opt/conda/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models==2.2.1) (20.11.0)\n","Requirement already satisfied: jsonpointer>1.13 in /opt/conda/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models==2.2.1) (2.4)\n","Requirement already satisfied: uri-template in /opt/conda/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models==2.2.1) (1.3.0)\n","Requirement already satisfied: webcolors>=1.11 in /opt/conda/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models==2.2.1) (1.13)\n","Requirement already satisfied: arrow>=0.15.0 in /opt/conda/lib/python3.10/site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models==2.2.1) (1.3.0)\n","Requirement already satisfied: types-python-dateutil>=2.8.10 in /opt/conda/lib/python3.10/site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets==7.5.1->contextualized_topic_models==2.2.1) (2.8.19.20240106)\n","Downloading contextualized_topic_models-2.2.1-py2.py3-none-any.whl (34 kB)\n","Downloading ipython-7.16.1-py3-none-any.whl (785 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m785.1/785.1 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading ipywidgets-7.5.1-py2.py3-none-any.whl (121 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ipykernel-5.5.6-py3-none-any.whl (121 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.0/121.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading widgetsnbextension-3.5.2-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n","\u001b[?25hDownloading backcall-0.2.0-py2.py3-none-any.whl (11 kB)\n","Installing collected packages: backcall, ipython, ipykernel, sentence-transformers, widgetsnbextension, ipywidgets, contextualized_topic_models\n","  Attempting uninstall: ipython\n","    Found existing installation: ipython 8.20.0\n","    Uninstalling ipython-8.20.0:\n","      Successfully uninstalled ipython-8.20.0\n","  Attempting uninstall: ipykernel\n","    Found existing installation: ipykernel 6.28.0\n","    Uninstalling ipykernel-6.28.0:\n","      Successfully uninstalled ipykernel-6.28.0\n","  Attempting uninstall: widgetsnbextension\n","    Found existing installation: widgetsnbextension 3.6.7\n","    Uninstalling widgetsnbextension-3.6.7:\n","      Successfully uninstalled widgetsnbextension-3.6.7\n","  Attempting uninstall: ipywidgets\n","    Found existing installation: ipywidgets 7.7.1\n","    Uninstalling ipywidgets-7.7.1:\n","      Successfully uninstalled ipywidgets-7.7.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","ipyleaflet 0.19.1 requires ipywidgets<9,>=7.6.0, but you have ipywidgets 7.5.1 which is incompatible.\n","ipympl 0.7.0 requires ipywidgets>=7.6.0, but you have ipywidgets 7.5.1 which is incompatible.\n","jupyter-console 6.6.3 requires ipykernel>=6.14, but you have ipykernel 5.5.6 which is incompatible.\n","jupyterlab 4.2.3 requires ipykernel>=6.5.0, but you have ipykernel 5.5.6 which is incompatible.\n","jupyterlab 4.2.3 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\n","jupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed backcall-0.2.0 contextualized_topic_models-2.2.1 ipykernel-5.5.6 ipython-7.16.1 ipywidgets-7.5.1 sentence-transformers-3.0.1 widgetsnbextension-3.5.2\n"]}],"source":["# !pip install wurlitzer\n","# !pip install keras==2.15.0\n","!pip install pytest>=5.4.3, pytest-cov>=2.6.1\n","!pip install mkdocs>=1.1, mkdocs-material>=4.6.3, mkdocstrings>=0.8.0\n","!pip install nltk>=3.2.4, srsly>=1.0.5 \n","#!pip install octis #==1.10.2 # - original paper\n","!pip install contextualized_topic_models==2.2.1"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-07-27T18:03:52.964902Z","iopub.status.busy":"2024-07-27T18:03:52.964385Z","iopub.status.idle":"2024-07-27T18:04:09.045429Z","shell.execute_reply":"2024-07-27T18:04:09.043847Z","shell.execute_reply.started":"2024-07-27T18:03:52.964841Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting bertopic==0.9.4\n","  Downloading bertopic-0.9.4-py2.py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: numpy>=1.20.0 in /opt/conda/lib/python3.10/site-packages (from bertopic==0.9.4) (1.26.4)\n","Collecting hdbscan>=0.8.27 (from bertopic==0.9.4)\n","  Downloading hdbscan-0.8.37-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n","Requirement already satisfied: umap-learn>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from bertopic==0.9.4) (0.5.6)\n","Requirement already satisfied: pandas>=1.1.5 in /opt/conda/lib/python3.10/site-packages (from bertopic==0.9.4) (2.2.2)\n","Requirement already satisfied: scikit-learn>=0.22.2.post1 in /opt/conda/lib/python3.10/site-packages (from bertopic==0.9.4) (1.2.2)\n","Requirement already satisfied: tqdm>=4.41.1 in /opt/conda/lib/python3.10/site-packages (from bertopic==0.9.4) (4.66.4)\n","Requirement already satisfied: sentence-transformers>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from bertopic==0.9.4) (3.0.1)\n","Requirement already satisfied: plotly>=4.7.0 in /opt/conda/lib/python3.10/site-packages (from bertopic==0.9.4) (5.18.0)\n","Requirement already satisfied: pyyaml<6.0 in /opt/conda/lib/python3.10/site-packages (from bertopic==0.9.4) (5.4.1)\n","Requirement already satisfied: cython<3,>=0.27 in /opt/conda/lib/python3.10/site-packages (from hdbscan>=0.8.27->bertopic==0.9.4) (0.29.37)\n","Requirement already satisfied: scipy>=1.0 in /opt/conda/lib/python3.10/site-packages (from hdbscan>=0.8.27->bertopic==0.9.4) (1.11.4)\n","Requirement already satisfied: joblib>=1.0 in /opt/conda/lib/python3.10/site-packages (from hdbscan>=0.8.27->bertopic==0.9.4) (1.4.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.5->bertopic==0.9.4) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.5->bertopic==0.9.4) (2023.3.post1)\n","Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.5->bertopic==0.9.4) (2023.4)\n","Requirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from plotly>=4.7.0->bertopic==0.9.4) (8.2.3)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from plotly>=4.7.0->bertopic==0.9.4) (21.3)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.22.2.post1->bertopic==0.9.4) (3.2.0)\n","Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=0.4.1->bertopic==0.9.4) (4.42.3)\n","Requirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=0.4.1->bertopic==0.9.4) (2.1.2+cpu)\n","Requirement already satisfied: huggingface-hub>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=0.4.1->bertopic==0.9.4) (0.23.4)\n","Requirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers>=0.4.1->bertopic==0.9.4) (9.5.0)\n","Requirement already satisfied: numba>=0.51.2 in /opt/conda/lib/python3.10/site-packages (from umap-learn>=0.5.0->bertopic==0.9.4) (0.58.1)\n","Requirement already satisfied: pynndescent>=0.5 in /opt/conda/lib/python3.10/site-packages (from umap-learn>=0.5.0->bertopic==0.9.4) (0.5.13)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic==0.9.4) (3.13.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic==0.9.4) (2024.5.0)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic==0.9.4) (2.32.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic==0.9.4) (4.9.0)\n","Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic==0.9.4) (0.41.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->plotly>=4.7.0->bertopic==0.9.4) (3.1.1)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic==0.9.4) (1.16.0)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic==0.9.4) (1.13.0)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic==0.9.4) (3.2.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic==0.9.4) (3.1.2)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers>=0.4.1->bertopic==0.9.4) (2023.12.25)\n","Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers>=0.4.1->bertopic==0.9.4) (0.4.3)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers>=0.4.1->bertopic==0.9.4) (0.19.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic==0.9.4) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic==0.9.4) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic==0.9.4) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic==0.9.4) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers>=0.4.1->bertopic==0.9.4) (2024.7.4)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic==0.9.4) (1.3.0)\n","Downloading bertopic-0.9.4-py2.py3-none-any.whl (57 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading hdbscan-0.8.37-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: hdbscan, bertopic\n","Successfully installed bertopic-0.9.4 hdbscan-0.8.37\n"]}],"source":["!pip install bertopic==0.9.4  # 0.9.4"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-07-26T21:51:31.124979Z","iopub.status.busy":"2024-07-26T21:51:31.124610Z","iopub.status.idle":"2024-07-26T21:51:31.130989Z","shell.execute_reply":"2024-07-26T21:51:31.129427Z","shell.execute_reply.started":"2024-07-26T21:51:31.124947Z"},"trusted":true},"outputs":[],"source":["# !pip install top2vec==1.0.26 #==1.0.26 1.0.34"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-07-27T18:04:09.048797Z","iopub.status.busy":"2024-07-27T18:04:09.048301Z","iopub.status.idle":"2024-07-27T18:04:37.748811Z","shell.execute_reply":"2024-07-27T18:04:37.747124Z","shell.execute_reply.started":"2024-07-27T18:04:09.048751Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting octis\n","  Downloading octis-1.14.0-py2.py3-none-any.whl.metadata (27 kB)\n","Requirement already satisfied: gensim<5.0,>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from octis) (4.3.2)\n","Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from octis) (3.2.4)\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from octis) (2.2.2)\n","Requirement already satisfied: spacy in /opt/conda/lib/python3.10/site-packages (from octis) (3.7.5)\n","Collecting scikit-learn==1.1.0 (from octis)\n","  Downloading scikit_learn-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n","Requirement already satisfied: scikit-optimize>=0.8.1 in /opt/conda/lib/python3.10/site-packages (from octis) (0.10.2)\n","Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from octis) (3.7.5)\n","Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from octis) (2.1.2+cpu)\n","Requirement already satisfied: numpy<2.0,>=1.23.0 in /opt/conda/lib/python3.10/site-packages (from octis) (1.26.4)\n","Collecting libsvm (from octis)\n","  Downloading libsvm-3.23.0.4.tar.gz (170 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.6/170.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: flask in /opt/conda/lib/python3.10/site-packages (from octis) (3.0.3)\n","Requirement already satisfied: sentence-transformers in /opt/conda/lib/python3.10/site-packages (from octis) (3.0.1)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from octis) (2.32.3)\n","Collecting tomotopy (from octis)\n","  Downloading tomotopy-0.12.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (29 kB)\n","Requirement already satisfied: scipy<1.13 in /opt/conda/lib/python3.10/site-packages (from octis) (1.11.4)\n","Requirement already satisfied: joblib>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.1.0->octis) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.1.0->octis) (3.2.0)\n","Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.10/site-packages (from gensim<5.0,>=4.2.0->octis) (6.4.0)\n","Requirement already satisfied: pyaml>=16.9 in /opt/conda/lib/python3.10/site-packages (from scikit-optimize>=0.8.1->octis) (24.4.0)\n","Requirement already satisfied: packaging>=21.3 in /opt/conda/lib/python3.10/site-packages (from scikit-optimize>=0.8.1->octis) (21.3)\n","Requirement already satisfied: Werkzeug>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from flask->octis) (3.0.3)\n","Requirement already satisfied: Jinja2>=3.1.2 in /opt/conda/lib/python3.10/site-packages (from flask->octis) (3.1.2)\n","Requirement already satisfied: itsdangerous>=2.1.2 in /opt/conda/lib/python3.10/site-packages (from flask->octis) (2.2.0)\n","Requirement already satisfied: click>=8.1.3 in /opt/conda/lib/python3.10/site-packages (from flask->octis) (8.1.7)\n","Requirement already satisfied: blinker>=1.6.2 in /opt/conda/lib/python3.10/site-packages (from flask->octis) (1.8.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->octis) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->octis) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->octis) (4.47.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->octis) (1.4.5)\n","Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->octis) (9.5.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->octis) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->octis) (2.9.0.post0)\n","Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk->octis) (1.16.0)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->octis) (2023.3.post1)\n","Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->octis) (2023.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->octis) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->octis) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->octis) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->octis) (2024.7.4)\n","Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers->octis) (4.42.3)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers->octis) (4.66.4)\n","Requirement already satisfied: huggingface-hub>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers->octis) (0.23.4)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->octis) (3.13.1)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->octis) (4.9.0)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->octis) (1.13.0)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->octis) (3.2.1)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->octis) (2024.5.0)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy->octis) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy->octis) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy->octis) (1.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy->octis) (2.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy->octis) (3.0.9)\n","Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /opt/conda/lib/python3.10/site-packages (from spacy->octis) (8.2.5)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy->octis) (1.1.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy->octis) (2.4.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy->octis) (2.0.10)\n","Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from spacy->octis) (0.4.1)\n","Requirement already satisfied: typer<1.0.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy->octis) (0.9.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy->octis) (2.5.3)\n","Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy->octis) (69.0.3)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy->octis) (3.4.0)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers->octis) (5.4.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from Jinja2>=3.1.2->flask->octis) (2.1.3)\n","Requirement already satisfied: language-data>=1.2 in /opt/conda/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy->octis) (1.2.0)\n","Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->octis) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy->octis) (2.14.6)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy->octis) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy->octis) (0.1.5)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers->octis) (2023.12.25)\n","Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers->octis) (0.4.3)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers->octis) (0.19.1)\n","Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy->octis) (0.18.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->octis) (1.3.0)\n","Requirement already satisfied: marisa-trie>=0.7.7 in /opt/conda/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy->octis) (1.2.0)\n","Downloading octis-1.14.0-py2.py3-none-any.whl (130 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.0/131.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading scikit_learn-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading tomotopy-0.12.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.2/17.2 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: libsvm\n","  Building wheel for libsvm (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for libsvm: filename=libsvm-3.23.0.4-cp310-cp310-linux_x86_64.whl size=184163 sha256=687fc87cb7f103c96ac3474f17146679d44d1c67f1fe206974133fb9b110bbee\n","  Stored in directory: /root/.cache/pip/wheels/79/c7/19/a8c85928f8e629654b8e1adb3c8091f0bb77344d0ee9954a85\n","Successfully built libsvm\n","Installing collected packages: tomotopy, libsvm, scikit-learn, octis\n","  Attempting uninstall: scikit-learn\n","    Found existing installation: scikit-learn 1.2.2\n","    Uninstalling scikit-learn-1.2.2:\n","      Successfully uninstalled scikit-learn-1.2.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","spaghetti 1.7.6 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\n","spopt 0.6.1 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed libsvm-3.23.0.4 octis-1.14.0 scikit-learn-1.1.0 tomotopy-0.12.7\n"]}],"source":["!pip install octis"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-07-27T18:04:37.751977Z","iopub.status.busy":"2024-07-27T18:04:37.751522Z","iopub.status.idle":"2024-07-27T18:04:37.849874Z","shell.execute_reply":"2024-07-27T18:04:37.848487Z","shell.execute_reply.started":"2024-07-27T18:04:37.751934Z"},"trusted":true},"outputs":[{"data":{"text/plain":["11"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["import gc\n","gc.collect()"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-07-27T18:04:37.852602Z","iopub.status.busy":"2024-07-27T18:04:37.852134Z","iopub.status.idle":"2024-07-27T18:04:46.856871Z","shell.execute_reply":"2024-07-27T18:04:46.855264Z","shell.execute_reply.started":"2024-07-27T18:04:37.852567Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["import re\n","import nltk\n","import string\n","import pandas as pd\n","import numpy as np\n","\n","from typing import List, Tuple, Union\n","from octis.dataset.dataset import Dataset\n","from octis.preprocessing.preprocessing import Preprocessing\n","\n","nltk.download(\"punkt\")\n","\n","\n","class DataLoader:\n","    \"\"\"Prepare and load custom data using OCTIS\n","\n","    Arguments:\n","        dataset: The name of the dataset, default options:\n","                    * trump\n","                    * 20news\n","\n","    Usage:\n","\n","    **Trump** - Unprocessed\n","\n","    ```python\n","    from evaluation import DataLoader\n","    dataloader = DataLoader(dataset=\"trump\").prepare_docs(save=\"trump.txt\").preprocess_octis(output_folder=\"trump\")\n","    ```\n","\n","    **20 Newsgroups** - Unprocessed\n","\n","    ```python\n","    from evaluation import DataLoader\n","    dataloader = DataLoader(dataset=\"20news\").prepare_docs(save=\"20news.txt\").preprocess_octis(output_folder=\"20news\")\n","    ```\n","\n","    **Custom Data**\n","\n","    Whenever you want to use a custom dataset (list of strings), make sure to use the loader like this:\n","\n","    ```python\n","    from evaluation import DataLoader\n","    dataloader = DataLoader(dataset=\"my_docs\").prepare_docs(save=\"my_docs.txt\", docs=my_docs).preprocess_octis(output_folder=\"my_docs\")\n","    ```\n","    \"\"\"\n","\n","    def __init__(self, dataset: str):\n","        self.dataset = dataset\n","        self.docs = None\n","        self.timestamps = None\n","        self.octis_docs = None\n","        self.doc_path = None\n","\n","    def load_docs(\n","        self, save: bool = False, docs: List[str] = None\n","    ) -> Tuple[List[str], Union[List[str], None]]:\n","        \"\"\"Load in the documents\n","\n","        ```python\n","        dataloader = DataLoader(dataset=\"trump\")\n","        docs, timestamps = dataloader.load_docs()\n","        ```\n","        \"\"\"\n","        if docs is not None:\n","            return self.docs, None\n","\n","        if self.dataset == \"trump\":\n","            self.docs, self.timestamps = self._trump()\n","        elif self.dataset == \"trump_dtm\":\n","            self.docs, self.timestamps = self._trump_dtm()\n","        elif self.dataset == \"un_dtm\":\n","            self.docs, self.timestamps = self._un_dtm()\n","        elif self.dataset == \"20news\":\n","            self.docs, self.timestamps = self._20news()\n","\n","        if save:\n","            self._save(self.docs, save)\n","\n","        return self.docs, self.timestamps\n","\n","    def load_octis(self, custom: bool = False) -> Dataset:\n","        \"\"\"Get dataset from OCTIS\n","\n","        Arguments:\n","            custom: Whether a custom dataset is used or one retrieved from\n","                    https://github.com/MIND-Lab/OCTIS#available-datasets\n","\n","        Usage:\n","\n","        ```python\n","        from evaluation import DataLoader\n","        dataloader = DataLoader(dataset=\"20news\")\n","        data = dataloader.load_octis(custom=True)\n","        ```\n","        \"\"\"\n","        data = Dataset()\n","\n","        if custom:\n","            data.load_custom_dataset_from_folder(self.dataset)\n","        else:\n","            data.fetch_dataset(self.dataset)\n","\n","        self.octis_docs = data\n","        return self.octis_docs\n","\n","    def prepare_docs(self, save: bool = False, docs: List[str] = None):\n","        \"\"\"Prepare documents\n","\n","        Arguments:\n","            save: The path to save the model to, make sure it ends in .json\n","            docs: The documents you want to preprocess in OCTIS\n","\n","        Usage:\n","\n","        ```python\n","        from evaluation import DataLoader\n","        dataloader = DataLoader(dataset=\"my_docs\").prepare_docs(save=\"my_docs.txt\", docs=my_docs)\n","        ```\n","        \"\"\"\n","        self.load_docs(save, docs)\n","        return self\n","\n","    def preprocess_octis(\n","        self,\n","        preprocessor: Preprocessing = None,\n","        documents_path: str = None,\n","        output_folder: str = \"docs\",\n","    ):\n","        \"\"\"Preprocess the data using OCTIS\n","\n","        Arguments:\n","            preprocessor: Custom OCTIS preprocessor\n","            documents_path: Path to the .txt file\n","            output_folder: Path to where you want to save the preprocessed data\n","\n","        Usage:\n","\n","        ```python\n","        from evaluation import DataLoader\n","        dataloader = DataLoader(dataset=\"my_docs\").prepare_docs(save=\"my_docs.txt\", docs=my_docs)\n","        dataloader.preprocess_octis(output_folder=\"my_docs\")\n","        ```\n","\n","        If you want to use your custom preprocessor:\n","\n","        ```python\n","        from evaluation import DataLoader\n","        from octis.preprocessing.preprocessing import Preprocessing\n","\n","        preprocessor = Preprocessing(lowercase=False,\n","                                remove_punctuation=False,\n","                                punctuation=string.punctuation,\n","                                remove_numbers=False,\n","                                lemmatize=False,\n","                                language='english',\n","                                split=False,\n","                                verbose=True,\n","                                save_original_indexes=True,\n","                                remove_stopwords_spacy=False)\n","\n","        dataloader = DataLoader(dataset=\"my_docs\").prepare_docs(save=\"my_docs.txt\", docs=my_docs)\n","        dataloader.preprocess_octis(preprocessor=preprocessor, output_folder=\"my_docs\")\n","        ```\n","        \"\"\"\n","        if preprocessor is None:\n","            preprocessor = Preprocessing(\n","                lowercase=False,\n","                remove_punctuation=False,\n","                punctuation=string.punctuation,\n","                remove_numbers=False,\n","                lemmatize=False,\n","                language=\"english\",\n","                split=False,\n","                verbose=True,\n","                save_original_indexes=True,\n","                remove_stopwords_spacy=False,\n","            )\n","        if not documents_path:\n","            documents_path = self.doc_path\n","        dataset = preprocessor.preprocess_dataset(documents_path=documents_path)\n","        dataset.save(output_folder)\n","\n","    def _trump(self) -> Tuple[List[str], List[str]]:\n","        \"\"\"Prepare the trump dataset\"\"\"\n","        trump = pd.read_csv(\n","            \"https://drive.google.com/uc?export=download&id=1xRKHaP-QwACMydlDnyFPEaFdtskJuBa6\"\n","        )\n","        trump = trump.loc[(trump.isRetweet == \"f\") & (trump.text != \"\"), :]\n","        timestamps = trump.date.to_list()\n","        docs = trump.text.to_list()\n","        docs = [doc.lower().replace(\"\\n\", \" \") for doc in docs if len(doc) > 2]\n","        timestamps = [\n","            timestamp for timestamp, doc in zip(timestamps, docs) if len(doc) > 2\n","        ]\n","        return docs, timestamps\n","\n","    def _trump_dtm(self) -> Tuple[List[str], List[str]]:\n","        \"\"\"Prepare the trump dataset including timestamps\"\"\"\n","        trump = pd.read_csv(\n","            \"https://drive.google.com/uc?export=download&id=1xRKHaP-QwACMydlDnyFPEaFdtskJuBa6\"\n","        )\n","        trump = trump.loc[(trump.isRetweet == \"f\") & (trump.text != \"\"), :]\n","        timestamps = trump.date.to_list()\n","        documents = trump.text.to_list()\n","\n","        docs = []\n","        time = []\n","        for doc, timestamp in zip(documents, timestamps):\n","            if len(doc) > 2:\n","                docs.append(doc.lower().replace(\"\\n\", \" \"))\n","                time.append(timestamp)\n","\n","        # Create bins\n","        nr_bins = 10\n","        df = pd.DataFrame({\"Doc\": docs, \"Timestamp\": time}).sort_values(\"Timestamp\")\n","        df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], infer_datetime_format=True)\n","        df[\"Bins\"] = pd.cut(df.Timestamp, bins=nr_bins)\n","        df[\"Timestamp\"] = df.apply(lambda row: row.Bins.left, 1)\n","        timestamps = df.Timestamp.tolist()\n","        documents = df.Doc.tolist()\n","\n","        return docs, timestamps\n","\n","    def _un_dtm(self) -> Tuple[List[str], List[str]]:\n","        \"\"\"Prepare the UN dataset\"\"\"\n","\n","        def create_paragraphs(text):\n","            text = text.replace(\"Mr.\\n\", \"Mr. \")\n","            text = text.replace(\".\\n\", \" \\p \")\n","            text = text.replace(\". \\n \", \" \\p \")\n","            text = text.replace(\". \\n\", \" \\p \")\n","            text = text.replace(\"\\n\", \" \")\n","            text = [x.strip().lower() for x in text.split(\"\\p\")]\n","            return text\n","\n","        dataset = pd.read_csv(\n","            \"https://runestone.academy/runestone/books/published/httlads/_static/un-general-debates.csv\"\n","        )\n","        dataset[\"text\"] = dataset.apply(lambda row: create_paragraphs(row.text), 1)\n","        dataset = dataset.explode(\"text\").sort_values(\"year\")\n","        dataset = dataset.loc[dataset.year > 2005, :] # original: > 2005\n","        # Set a random seed for reproducibility\n","        np.random.seed(42)\n","\n","        # Define a function to sample at most 2000 entries per year\n","        def sample_yearly(df, year_column, max_samples):\n","            return df.groupby(year_column).apply(lambda x: x.sample(min(len(x), max_samples))).reset_index(drop=True)\n","\n","        dataset = sample_yearly(dataset, 'year', 2000)\n","        \n","        docs = dataset.text.tolist()\n","        timestamps = dataset.year.tolist()\n","        return docs, timestamps\n","\n","    def _save(self, docs: List[str], save: str):\n","        \"\"\"Save the documents\"\"\"\n","        with open(save, mode=\"wt\", encoding=\"utf-8\") as myfile:\n","            myfile.write(\"\\n\".join(docs))\n","\n","        self.doc_path = save"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluations"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-07-27T18:04:46.859824Z","iopub.status.busy":"2024-07-27T18:04:46.859064Z","iopub.status.idle":"2024-07-27T18:06:22.725260Z","shell.execute_reply":"2024-07-27T18:06:22.723726Z","shell.execute_reply.started":"2024-07-27T18:04:46.859779Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-07-27 18:05:10.726655: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-07-27 18:05:10.726835: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-07-27 18:05:10.889279: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]},{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["import json\n","import time\n","import itertools\n","import pandas as pd\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from typing import Mapping, Any, List, Tuple\n","\n","try:\n","    from bertopic import BERTopic\n","except ImportError:\n","    pass\n","\n","try:\n","    from top2vec import Top2Vec\n","except ImportError:\n","    pass\n","\n","try:\n","    from contextualized_topic_models.models.ctm import CombinedTM\n","    from contextualized_topic_models.utils.data_preparation import (\n","        TopicModelDataPreparation,\n","    )\n","    import nltk\n","\n","    nltk.download(\"stopwords\")\n","    from nltk.corpus import stopwords\n","except ImportError:\n","    pass\n","\n","from octis.models.ETM import ETM\n","from octis.models.LDA import LDA\n","from octis.models.NMF import NMF\n","from octis.models.CTM import CTM\n","from octis.dataset.dataset import Dataset\n","from octis.evaluation_metrics.diversity_metrics import TopicDiversity\n","from octis.evaluation_metrics.coherence_metrics import Coherence\n","\n","import gensim\n","import gensim.corpora as corpora\n","from gensim.models import ldaseqmodel\n","\n","\n","class Trainer:\n","\n","    def __init__(\n","        self,\n","        dataset: str,\n","        model_name: str,\n","        params: Mapping[str, Any],\n","        topk: int = 10,\n","        custom_dataset: bool = False,\n","        bt_embeddings: np.ndarray = None,\n","        bt_timestamps: List[str] = None,\n","        bt_nr_bins: int = None,\n","        custom_model=None,\n","        verbose: bool = True,\n","    ):\n","        self.dataset = dataset\n","        self.custom_dataset = custom_dataset\n","        self.model_name = model_name\n","        self.params = params\n","        self.topk = topk\n","        self.timestamps = bt_timestamps\n","        self.nr_bins = bt_nr_bins\n","        self.embeddings = bt_embeddings\n","        self.ctm_preprocessed_docs = None\n","        self.custom_model = custom_model\n","        self.verbose = verbose\n","\n","        # Prepare data and metrics\n","        self.data = self.get_dataset()\n","        self.metrics = self.get_metrics()\n","\n","        # CTM\n","        self.qt_ctm = None\n","        self.training_dataset_ctm = None\n","\n","    def train(self, save: str = False) -> Mapping[str, Any]:\n","        \"\"\"Train a topic model\n","\n","        Arguments:\n","            save: The name of the file to save it to.\n","                  It will be saved as a .json in the current\n","                  working directory\n","\n","        Usage:\n","\n","        ```python\n","        from evaluation import Trainer\n","        dataset, custom = \"20NewsGroup\", False\n","        params = {\"num_topics\": [(i+1)*10 for i in range(5)], \"random_state\": 42}\n","\n","        trainer = Trainer(dataset=dataset,\n","                        model_name=\"LDA\",\n","                        params=params,\n","                        custom_dataset=custom,\n","                        verbose=True)\n","        results = trainer.train(save=\"LDA_results\")\n","        ```\n","        \"\"\"\n","\n","        results = []\n","\n","        # Loop over all parameters\n","        params_name = list(self.params.keys())\n","        params = {\n","            param: (value if type(value) == list else [value])\n","            for param, value in self.params.items()\n","        }\n","        new_params = list(itertools.product(*params.values()))\n","        for param_combo in new_params:\n","\n","            # Train and evaluate model\n","            params_to_use = {\n","                param: value for param, value in zip(params_name, param_combo)\n","            }\n","            output, computation_time = self._train_tm_model(params_to_use)\n","            scores = self.evaluate(output)\n","\n","            # Update results\n","            result = {\n","                \"Dataset\": self.dataset,\n","                \"Dataset Size\": len(self.data.get_corpus()),\n","                \"Model\": self.model_name,\n","                \"Params\": params_to_use,\n","                \"Scores\": scores,\n","                \"Computation Time\": computation_time,\n","            }\n","            results.append(result)\n","\n","        if save:\n","            with open(f\"{save}.json\", \"w\") as f:\n","                json.dump(results, f)\n","\n","            try:\n","                from google.colab import files\n","\n","                files.download(f\"{save}.json\")\n","            except ImportError:\n","                pass\n","\n","        return results\n","\n","    def _train_tm_model(\n","        self, params: Mapping[str, Any]\n","    ) -> Tuple[Mapping[str, Any], float]:\n","        \"\"\"Select and train the Topic Model\"\"\"\n","        # Train custom CTM\n","        if self.model_name == \"CTM_CUSTOM\":\n","            if self.qt_ctm is None:\n","                self._preprocess_ctm()\n","            return self._train_ctm(params)\n","\n","        # Train BERTopic\n","        elif \"BERTopic\" in self.model_name: # MODIFIED\n","            return self._train_bertopic(params)\n","\n","        # Train Top2Vec\n","        elif self.model_name == \"Top2Vec\":\n","            return self._train_top2vec(params)\n","\n","        # Train LDAseq\n","        elif self.model_name == \"LDAseq\":\n","            return self._train_ldaseq(params)\n","\n","        # Train OCTIS model\n","        octis_models = [\"ETM\", \"LDA\", \"CTM\", \"NMF\"]\n","        if self.model_name in octis_models:\n","            return self._train_octis_model(params)\n","\n","    def _train_ldaseq(\n","        self, params: Mapping[str, any]\n","    ) -> Tuple[Mapping[str, Any], float]:\n","        \"\"\"Train LDA seq model\"\"\"\n","        data = self.data.get_corpus()\n","        docs = [\" \".join(words) for words in data]\n","\n","        df = pd.DataFrame({\"Doc\": docs, \"Timestamp\": self.timestamps}).sort_values(\n","            \"Timestamp\"\n","        )\n","        df[\"Bins\"] = pd.cut(df.Timestamp, bins=params[\"nr_bins\"])\n","        df[\"Timestamp\"] = df.apply(lambda row: row.Bins.left, 1)\n","        timestamps = df.groupby(\"Bins\").count().Timestamp.values\n","        docs = df.Doc.values\n","\n","        data_words = list(sent_to_words(docs))\n","        id2word = corpora.Dictionary(data_words)\n","        corpus = [id2word.doc2bow(text) for text in data_words]\n","\n","        print(len(corpus), len(self.timestamps), timestamps)\n","\n","        params[\"corpus\"] = corpus\n","        params[\"id2word\"] = id2word\n","        params[\"time_slice\"] = timestamps\n","        del params[\"nr_bins\"]\n","        \n","        import logging\n","        from gensim.corpora.dictionary import Dictionary\n","        logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n","\n","        start = time.time()\n","        ldaseq = ldaseqmodel.LdaSeqModel(**params)\n","        \n","        # Manually track and log time slices\n","        current_time_slice = 0\n","        doc_counter = 0\n","        for doc in corpus:\n","            doc_counter += 1\n","            if doc_counter > sum(timestamps[:current_time_slice+1]):\n","                current_time_slice += 1\n","            \n","            logging.info(f'Processing document {doc_counter}, current time slice: {current_time_slice}')\n","        \n","        \n","        end = time.time()\n","        computation_time = end - start\n","\n","        all_topics = {}\n","        for i in range(len(timestamps)):\n","            topics = ldaseq.print_topics(time=i)\n","            topics = [[word for word, _ in topic][:5] for topic in topics]\n","            all_topics[i] = {\"topics\": topics}\n","\n","        return all_topics, computation_time\n","\n","    def _train_top2vec(\n","        self, params: Mapping[str, Any]\n","    ) -> Tuple[Mapping[str, Any], float]:\n","        \"\"\"Train Top2Vec\"\"\"\n","        nr_topics = None\n","        data = self.data.get_corpus()\n","        data = [\" \".join(words) for words in data]\n","        params[\"documents\"] = data\n","\n","        if params.get(\"nr_topics\"):\n","            nr_topics = params[\"nr_topics\"]\n","            del params[\"nr_topics\"]\n","\n","        start = time.time()\n","\n","        if self.custom_model is not None:\n","            model = self.custom_model(**params)\n","        else:\n","            model = Top2Vec(**params)\n","\n","        if nr_topics:\n","            try:\n","                _ = model.hierarchical_topic_reduction(nr_topics)\n","                params[\"reduction\"] = True\n","                params[\"nr_topics\"] = nr_topics\n","            except:\n","                params[\"reduction\"] = False\n","                nr_topics = False\n","\n","        end = time.time()\n","        computation_time = float(end - start)\n","\n","        if nr_topics:\n","            topic_words, _, _ = model.get_topics(reduced=True)\n","        else:\n","            topic_words, _, _ = model.get_topics(reduced=False)\n","\n","        topics_old = [list(topic[:10]) for topic in topic_words]\n","        all_words = [word for words in self.data.get_corpus() for word in words]\n","        topics = []\n","        for topic in topics_old:\n","            words = []\n","            for word in topic:\n","                if word in all_words:\n","                    words.append(word)\n","                else:\n","                    print(f\"error: {word}\")\n","                    words.append(all_words[0])\n","            topics.append(words)\n","\n","        if not nr_topics:\n","            params[\"nr_topics\"] = len(topics)\n","            params[\"reduction\"] = False\n","\n","        del params[\"documents\"]\n","        output_tm = {\n","            \"topics\": topics,\n","        }\n","        return output_tm, computation_time\n","\n","    def _train_ctm(self, params) -> Tuple[Mapping[str, Any], float]:\n","        \"\"\"Train CTM\"\"\"\n","        params[\"bow_size\"] = len(self.qt_ctm.vocab)\n","        ctm = CombinedTM(**params)\n","\n","        start = time.time()\n","        ctm.fit(self.training_dataset_ctm)\n","        end = time.time()\n","        computation_time = float(end - start)\n","\n","        topics = ctm.get_topics(10)\n","        topics = [topics[x] for x in topics]\n","\n","        output_tm = {\n","            \"topics\": topics,\n","        }\n","\n","        return output_tm, computation_time\n","\n","    def _preprocess_ctm(self):\n","        \"\"\"Preprocess data for CTM\"\"\"\n","        # Prepare docs\n","        data = self.data.get_corpus()\n","        docs = [\" \".join(words) for words in data]\n","\n","        # Remove stop words\n","        stop_words = stopwords.words(\"english\")\n","        preprocessed_documents = [\n","            \" \".join([x for x in doc.split(\" \") if x not in stop_words]).strip()\n","            for doc in docs\n","        ]\n","\n","        # Get vocabulary\n","        vectorizer = CountVectorizer(\n","            max_features=2000, token_pattern=r\"\\b[a-zA-Z]{2,}\\b\"\n","        )\n","        vectorizer.fit_transform(preprocessed_documents)\n","        # vocabulary = set(vectorizer.get_feature_names())\n","        try:\n","            vocabulary = set(vectorizer.get_feature_names_out())\n","        except AttributeError:\n","            vocabulary = set(vectorizer.get_feature_names())\n","            \n","        # Preprocess documents further\n","        preprocessed_documents = [\n","            \" \".join([w for w in doc.split() if w in vocabulary]).strip()\n","            for doc in preprocessed_documents\n","        ]\n","\n","        # Prepare CTM data\n","        qt = TopicModelDataPreparation(\"all-mpnet-base-v2\")\n","        training_dataset = qt.fit(\n","            text_for_contextual=docs, text_for_bow=preprocessed_documents\n","        )\n","\n","        self.qt_ctm = qt\n","        self.training_dataset_ctm = training_dataset\n","\n","    def _train_octis_model(\n","        self, params: Mapping[str, any]\n","    ) -> Tuple[Mapping[str, Any], float]:\n","        \"\"\"Train OCTIS model\"\"\"\n","\n","        if self.model_name == \"ETM\":\n","            model = ETM(**params)\n","            model.use_partitions = False\n","        elif self.model_name == \"LDA\":\n","            model = LDA(**params)\n","            model.use_partitions = False\n","        elif self.model_name == \"CTM\":\n","            model = CTM(**params)\n","            model.use_partitions = False\n","        elif self.model_name == \"NMF\":\n","            model = NMF(**params)\n","            model.use_partitions = False\n","\n","        start = time.time()\n","        output_tm = model.train_model(self.data)\n","        end = time.time()\n","        computation_time = end - start\n","        return output_tm, computation_time\n","\n","    def _train_bertopic(\n","        self, params: Mapping[str, any]\n","    ) -> Tuple[Mapping[str, Any], float]:\n","        \"\"\"Train BERTopic model\"\"\"\n","        data = self.data.get_corpus()\n","        data = [\" \".join(words) for words in data]\n","        params[\"calculate_probabilities\"] = False\n","        \n","        if self.custom_model is not None:\n","            model = self.custom_model(**params)\n","        else:\n","            model = BERTopic(**params)\n","\n","        start = time.time()\n","        topics, _ = model.fit_transform(data, self.embeddings)\n","        \n","\n","        # Dynamic Topic Modeling\n","        if self.timestamps:\n","            topics_over_time = model.topics_over_time(\n","                data,\n","                topics,\n","                self.timestamps,\n","                nr_bins=self.nr_bins,\n","                evolution_tuning=False,\n","                global_tuning=False,\n","            )\n","            unique_timestamps = topics_over_time.Timestamp.unique()\n","            dtm_topics = {}\n","            for unique_timestamp in unique_timestamps:\n","                dtm_topic = topics_over_time.loc[\n","                    topics_over_time.Timestamp == unique_timestamp, :\n","                ].sort_values(\"Frequency\", ascending=True)\n","                dtm_topic = dtm_topic.loc[dtm_topic.Topic != -1, :]\n","                dtm_topic = [topic.split(\", \") for topic in dtm_topic.Words.values]\n","                dtm_topics[unique_timestamp] = {\"topics\": dtm_topic}\n","\n","                all_words = [word for words in self.data.get_corpus() for word in words]\n","\n","                updated_topics = []\n","                for topic in dtm_topic:\n","                    updated_topic = []\n","                    for word in topic:\n","                        if word not in all_words:\n","                            print(word)\n","                            updated_topic.append(all_words[0])\n","                        else:\n","                            updated_topic.append(word)\n","                    updated_topics.append(updated_topic)\n","\n","                dtm_topics[unique_timestamp] = {\"topics\": updated_topics}\n","\n","            output_tm = dtm_topics\n","\n","        end = time.time()\n","        computation_time = float(end - start)\n","\n","        if not self.timestamps:\n","            all_words = [word for words in self.data.get_corpus() for word in words]\n","            bertopic_topics = [\n","                [\n","                    vals[0] if vals[0] in all_words else all_words[0]\n","                    for vals in model.get_topic(i)[:10]\n","                ]\n","                for i in range(len(set(topics)) - 1)\n","            ]\n","\n","            output_tm = {\"topics\": bertopic_topics}\n","\n","        return output_tm, computation_time\n","\n","    def evaluate(self, output_tm):\n","        \"\"\"Using metrics and output of the topic model, evaluate the topic model\"\"\"\n","        if self.timestamps:\n","            results = {str(timestamp): {} for timestamp, _ in output_tm.items()}\n","            for timestamp, topics in output_tm.items():\n","                self.metrics = self.get_metrics()\n","                for scorers, _ in self.metrics:\n","                    for scorer, name in scorers:\n","                        score = scorer.score(topics)\n","                        results[str(timestamp)][name] = float(score)\n","\n","        else:\n","            # Calculate results\n","            results = {}\n","            for scorers, _ in self.metrics:\n","                for scorer, name in scorers:\n","                    score = scorer.score(output_tm)\n","                    results[name] = float(score)\n","\n","            # Print results\n","            if self.verbose:\n","                print(\"Results\")\n","                print(\"============\")\n","                for metric, score in results.items():\n","                    print(f\"{metric}: {str(score)}\")\n","                print(\" \")\n","\n","        return results\n","\n","    def get_dataset(self):\n","        \"\"\"Get dataset from OCTIS\"\"\"\n","        data = Dataset()\n","\n","        if self.custom_dataset:\n","            data.load_custom_dataset_from_folder(self.dataset)\n","        else:\n","            data.fetch_dataset(self.dataset)\n","        return data\n","\n","    def get_metrics(self):\n","        \"\"\"Prepare evaluation measures using OCTIS\"\"\"\n","        npmi = Coherence(texts=self.data.get_corpus(), topk=self.topk, measure=\"c_npmi\")\n","        topic_diversity = TopicDiversity(topk=self.topk)\n","\n","        # Define methods\n","        coherence = [(npmi, \"npmi\")]\n","        diversity = [(topic_diversity, \"diversity\")]\n","        metrics = [(coherence, \"Coherence\"), (diversity, \"Diversity\")]\n","\n","        return metrics\n","\n","\n","def sent_to_words(sentences):\n","    for sentence in sentences:\n","        yield (gensim.utils.simple_preprocess(str(sentence), deacc=True))"]},{"cell_type":"markdown","metadata":{},"source":["## Data"]},{"cell_type":"markdown","metadata":{},"source":["### Trump data"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-07-27T18:06:22.821177Z","iopub.status.busy":"2024-07-27T18:06:22.820677Z","iopub.status.idle":"2024-07-27T18:06:33.922889Z","shell.execute_reply":"2024-07-27T18:06:33.921405Z","shell.execute_reply.started":"2024-07-27T18:06:22.821137Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 46693/46693 [00:00<00:00, 235100.83it/s]\n"]},{"name":"stdout","output_type":"stream","text":["created vocab\n","53637\n","words filtering done\n","CPU times: user 3.36 s, sys: 141 ms, total: 3.5 s\n","Wall time: 11.1 s\n"]}],"source":["%%time\n","dataloader = DataLoader(dataset=\"trump\").prepare_docs(save=\"trump.txt\").preprocess_octis(output_folder=\"trump\")"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-07-27T18:06:33.924963Z","iopub.status.busy":"2024-07-27T18:06:33.924527Z","iopub.status.idle":"2024-07-27T18:06:57.991777Z","shell.execute_reply":"2024-07-27T18:06:57.990301Z","shell.execute_reply.started":"2024-07-27T18:06:33.924932Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 46693/46693 [00:00<00:00, 235766.51it/s]\n"]},{"name":"stdout","output_type":"stream","text":["created vocab\n","53637\n","words filtering done\n","CPU times: user 5.19 s, sys: 105 ms, total: 5.29 s\n","Wall time: 24.1 s\n"]}],"source":["%%time\n","dataloader = DataLoader(dataset=\"trump_dtm\").prepare_docs(save=\"trump_dtm.txt\").preprocess_octis(output_folder=\"trump_dtm\")"]},{"cell_type":"markdown","metadata":{},"source":["### United Nations data"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-07-27T18:06:57.994478Z","iopub.status.busy":"2024-07-27T18:06:57.993764Z","iopub.status.idle":"2024-07-27T18:07:08.514825Z","shell.execute_reply":"2024-07-27T18:07:08.513284Z","shell.execute_reply.started":"2024-07-27T18:06:57.994429Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_33/3565351604.py:248: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  return df.groupby(year_column).apply(lambda x: x.sample(min(len(x), max_samples))).reset_index(drop=True)\n","100%|██████████| 20000/20000 [00:00<00:00, 82675.95it/s]\n"]},{"name":"stdout","output_type":"stream","text":["created vocab\n","23029\n","words filtering done\n","CPU times: user 7.05 s, sys: 584 ms, total: 7.63 s\n","Wall time: 10.5 s\n"]}],"source":["%%time\n","dataloader = DataLoader(dataset=\"un_dtm\").prepare_docs(save=\"un_dtm.txt\").preprocess_octis(output_folder=\"un_dtm\")"]},{"cell_type":"markdown","metadata":{},"source":["## Model Evaluations\n","* All evaluations are carried out in Kaggle notebook thus were contrained  by the Kaggle's 12-hour training limit, thus we could not finish running LDA sequence model. Similarly, CTM evaluations for 20NewsGroup and BBC News datasets are also skipped.\n","* We skipped all models using `Top2Vec` or `Doc2Vec` models due to unsolved conflicts caused by the older version of the `gensim` library.\n","\n","* We did not test Wall Time of models as packages/libraries have changed significantly in the last two years, so the results won't be comparable with the original paper."]},{"cell_type":"markdown","metadata":{},"source":["### Trump Data"]},{"cell_type":"markdown","metadata":{},"source":["#### Trump - NMF(CPU)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-07-25T16:36:37.461277Z","iopub.status.busy":"2024-07-25T16:36:37.460609Z","iopub.status.idle":"2024-07-25T17:02:08.391103Z","shell.execute_reply":"2024-07-25T17:02:08.390212Z","shell.execute_reply.started":"2024-07-25T16:36:37.461240Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Random State 0\n","Results\n","============\n","npmi: -0.005742839116935676\n","diversity: 0.4\n"," \n","Results\n","============\n","npmi: 0.007421120689733071\n","diversity: 0.42\n"," \n","Results\n","============\n","npmi: 0.014766098200964957\n","diversity: 0.3933333333333333\n"," \n","Results\n","============\n","npmi: 0.016208854270394913\n","diversity: 0.3375\n"," \n","Results\n","============\n","npmi: 0.017222793405267747\n","diversity: 0.348\n"," \n","Random State 21\n","Results\n","============\n","npmi: -0.003917059704810471\n","diversity: 0.46\n"," \n","Results\n","============\n","npmi: 0.007975363716605934\n","diversity: 0.415\n"," \n","Results\n","============\n","npmi: 0.02100240700031533\n","diversity: 0.38\n"," \n","Results\n","============\n","npmi: 0.010374395636827632\n","diversity: 0.36\n"," \n","Results\n","============\n","npmi: 0.006939519775300629\n","diversity: 0.316\n"," \n","Random State 42\n","Results\n","============\n","npmi: -0.004374876532201073\n","diversity: 0.38\n"," \n","Results\n","============\n","npmi: 0.006425754668690988\n","diversity: 0.405\n"," \n","Results\n","============\n","npmi: 0.01103688653374102\n","diversity: 0.37666666666666665\n"," \n","Results\n","============\n","npmi: 0.01675117935273563\n","diversity: 0.3575\n"," \n","Results\n","============\n","npmi: 0.015734180346554372\n","diversity: 0.342\n"," \n","Training COMPLETED\n"]}],"source":["for i, random_state in enumerate([0, 21, 42]):\n","    print(\"Random State\", random_state)\n","    dataset, custom = \"trump\", True\n","    params = {\"num_topics\": [(i+1)*10 for i in range(5)], \"random_state\": random_state}\n","\n","    trainer = Trainer(dataset=dataset,\n","                      model_name=\"NMF\",\n","                      params=params,\n","                      custom_dataset=custom,\n","                      verbose=True)\n","    results = trainer.train(save=f\"NMF_trump_{i+1}\")\n","    \n","print(\"Training COMPLETED\")"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-07-25T17:02:10.021295Z","iopub.status.busy":"2024-07-25T17:02:10.020943Z","iopub.status.idle":"2024-07-25T17:02:10.498462Z","shell.execute_reply":"2024-07-25T17:02:10.497414Z","shell.execute_reply.started":"2024-07-25T17:02:10.021266Z"},"trusted":true},"outputs":[{"data":{"text/plain":["32"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["gc.collect()"]},{"cell_type":"markdown","metadata":{},"source":["#### Trump - LDA (CPU)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-07-25T17:02:54.540061Z","iopub.status.busy":"2024-07-25T17:02:54.539193Z","iopub.status.idle":"2024-07-25T17:15:16.792140Z","shell.execute_reply":"2024-07-25T17:15:16.791233Z","shell.execute_reply.started":"2024-07-25T17:02:54.540028Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Random State 0\n","Results\n","============\n","npmi: -0.0069114454748685815\n","diversity: 0.48\n"," \n","Results\n","============\n","npmi: -0.005003465267478509\n","diversity: 0.425\n"," \n","Results\n","============\n","npmi: -0.004333717218804874\n","diversity: 0.5133333333333333\n"," \n","Results\n","============\n","npmi: -0.013484627735113936\n","diversity: 0.5475\n"," \n","Results\n","============\n","npmi: -0.02873932987802867\n","diversity: 0.556\n"," \n","Random State 21\n","Results\n","============\n","npmi: -0.00873283471860894\n","diversity: 0.41\n"," \n","Results\n","============\n","npmi: -0.0032195570281142545\n","diversity: 0.44\n"," \n","Results\n","============\n","npmi: -0.011184204528511066\n","diversity: 0.52\n"," \n","Results\n","============\n","npmi: -0.010834766671676588\n","diversity: 0.5275\n"," \n","Results\n","============\n","npmi: -0.020161391882537755\n","diversity: 0.596\n"," \n","Random State 42\n","Results\n","============\n","npmi: -0.004250980433560532\n","diversity: 0.45\n"," \n","Results\n","============\n","npmi: -0.006889694250977099\n","diversity: 0.47\n"," \n","Results\n","============\n","npmi: -0.005643503573800876\n","diversity: 0.48333333333333334\n"," \n","Results\n","============\n","npmi: -0.011009338092369111\n","diversity: 0.55\n"," \n","Results\n","============\n","npmi: -0.018123237177004415\n","diversity: 0.574\n"," \n","Training COMPLETED\n"]}],"source":["for i, random_state in enumerate([0, 21, 42]):\n","    print(\"Random State\", random_state)\n","    dataset, custom = \"trump\", True\n","    params = {\"num_topics\": [(i+1)*10 for i in range(5)], \"random_state\": random_state}\n","\n","    trainer = Trainer(dataset=dataset,\n","                      model_name=\"LDA\",\n","                      params=params,\n","                      custom_dataset=custom,\n","                      verbose=True)\n","    results = trainer.train(save=f\"LDA_trump_{i+1}\")\n","\n","print(\"Training COMPLETED\")"]},{"cell_type":"markdown","metadata":{},"source":["### Trump - CTM (GPU)\n","\n","For CTM, we only trained it twice due to the 12-hour training limit on Kaggle."]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-07-24T22:09:15.220598Z","iopub.status.busy":"2024-07-24T22:09:15.220211Z","iopub.status.idle":"2024-07-24T22:09:15.227566Z","shell.execute_reply":"2024-07-24T22:09:15.226414Z","shell.execute_reply.started":"2024-07-24T22:09:15.220568Z"},"trusted":true},"outputs":[],"source":["if not hasattr(CountVectorizer, 'get_feature_names'):\n","    CountVectorizer.get_feature_names = CountVectorizer.get_feature_names_out"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for i in range(2):\n","    print(\"*\"*60)\n","    print(\"Round\", i)\n","    dataset, custom = \"trump\", True\n","    params = {\n","        \"n_components\": [(i+1)*10 for i in range(5)],\n","        \"contextual_size\":768\n","    }\n","\n","    trainer = Trainer(dataset=dataset,\n","                      model_name=\"CTM_CUSTOM\",\n","                      params=params,\n","                      custom_dataset=custom,\n","                      verbose=True)\n","    results = trainer.train(save=f\"CTM_trump_{i+1}\")\n","\n","print(\"Training COMPLETED\")"]},{"cell_type":"markdown","metadata":{},"source":["#### Trump - BERTopic (CPU): `all-mpnet-base-v2`"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-07-25T17:42:46.869895Z","iopub.status.busy":"2024-07-25T17:42:46.869469Z","iopub.status.idle":"2024-07-25T17:43:37.291117Z","shell.execute_reply":"2024-07-25T17:43:37.290335Z","shell.execute_reply.started":"2024-07-25T17:42:46.869863Z"},"trusted":true},"outputs":[],"source":["%%capture\n","from sentence_transformers import SentenceTransformer\n","\n","# Prepare data\n","dataset, custom = \"trump\", True\n","data_loader = DataLoader(dataset)\n","_, timestamps = data_loader.load_docs()\n","data = data_loader.load_octis(custom)\n","data = [\" \".join(words) for words in data.get_corpus()]\n","\n","# Extract embeddings\n","model = SentenceTransformer(\"all-mpnet-base-v2\")\n","embeddings = model.encode(data, show_progress_bar=True)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-07-25T17:43:37.293033Z","iopub.status.busy":"2024-07-25T17:43:37.292739Z","iopub.status.idle":"2024-07-25T17:57:57.516438Z","shell.execute_reply":"2024-07-25T17:57:57.515342Z","shell.execute_reply.started":"2024-07-25T17:43:37.293008Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["ROUND 0\n"]},{"name":"stderr","output_type":"stream","text":["2024-07-25 17:44:33,399 - BERTopic - Reduced dimensionality with UMAP\n","/opt/conda/lib/python3.10/site-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n","  pid = os.fork()\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","2024-07-25 17:44:38,158 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n","2024-07-25 17:44:45,719 - BERTopic - Reduced number of topics from 370 to 11\n"]},{"name":"stdout","output_type":"stream","text":["Results\n","============\n","npmi: 0.05332787543368359\n","diversity: 0.64\n"," \n"]},{"name":"stderr","output_type":"stream","text":["2024-07-25 17:45:30,228 - BERTopic - Reduced dimensionality with UMAP\n","2024-07-25 17:45:33,638 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n","2024-07-25 17:45:40,930 - BERTopic - Reduced number of topics from 356 to 21\n"]},{"name":"stdout","output_type":"stream","text":["Results\n","============\n","npmi: 0.06394426641115683\n","diversity: 0.64\n"," \n"]},{"name":"stderr","output_type":"stream","text":["2024-07-25 17:46:25,554 - BERTopic - Reduced dimensionality with UMAP\n","2024-07-25 17:46:28,755 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n","2024-07-25 17:46:36,218 - BERTopic - Reduced number of topics from 364 to 31\n"]},{"name":"stdout","output_type":"stream","text":["Results\n","============\n","npmi: 0.06451235881119809\n","diversity: 0.6666666666666666\n"," \n"]},{"name":"stderr","output_type":"stream","text":["2024-07-25 17:47:21,949 - BERTopic - Reduced dimensionality with UMAP\n","2024-07-25 17:47:25,305 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n","2024-07-25 17:47:32,697 - BERTopic - Reduced number of topics from 363 to 41\n"]},{"name":"stdout","output_type":"stream","text":["Results\n","============\n","npmi: 0.06619399365631526\n","diversity: 0.6675\n"," \n"]},{"name":"stderr","output_type":"stream","text":["2024-07-25 17:48:17,963 - BERTopic - Reduced dimensionality with UMAP\n","2024-07-25 17:48:21,304 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n","2024-07-25 17:48:28,765 - BERTopic - Reduced number of topics from 374 to 51\n"]},{"name":"stdout","output_type":"stream","text":["Results\n","============\n","npmi: 0.07314498744638756\n","diversity: 0.724\n"," \n","ROUND 1\n"]},{"name":"stderr","output_type":"stream","text":["2024-07-25 17:49:16,219 - BERTopic - Reduced dimensionality with UMAP\n","2024-07-25 17:49:19,610 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n","2024-07-25 17:49:27,122 - BERTopic - Reduced number of topics from 373 to 11\n"]},{"name":"stdout","output_type":"stream","text":["Results\n","============\n","npmi: 0.06270095712188384\n","diversity: 0.68\n"," \n"]},{"name":"stderr","output_type":"stream","text":["2024-07-25 17:50:11,127 - BERTopic - Reduced dimensionality with UMAP\n","2024-07-25 17:50:14,532 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n","2024-07-25 17:50:22,009 - BERTopic - Reduced number of topics from 369 to 21\n"]},{"name":"stdout","output_type":"stream","text":["Results\n","============\n","npmi: 0.066548253851995\n","diversity: 0.625\n"," \n"]},{"name":"stderr","output_type":"stream","text":["2024-07-25 17:51:06,564 - BERTopic - Reduced dimensionality with UMAP\n","2024-07-25 17:51:09,994 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n","2024-07-25 17:51:17,657 - BERTopic - Reduced number of topics from 373 to 31\n"]},{"name":"stdout","output_type":"stream","text":["Results\n","============\n","npmi: 0.07297297154844692\n","diversity: 0.6533333333333333\n"," \n"]},{"name":"stderr","output_type":"stream","text":["2024-07-25 17:52:01,881 - BERTopic - Reduced dimensionality with UMAP\n","2024-07-25 17:52:05,197 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n","2024-07-25 17:52:12,614 - BERTopic - Reduced number of topics from 372 to 41\n"]},{"name":"stdout","output_type":"stream","text":["Results\n","============\n","npmi: 0.07463074095990775\n","diversity: 0.695\n"," \n"]},{"name":"stderr","output_type":"stream","text":["2024-07-25 17:52:56,895 - BERTopic - Reduced dimensionality with UMAP\n","2024-07-25 17:53:00,214 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n","2024-07-25 17:53:07,378 - BERTopic - Reduced number of topics from 358 to 51\n"]},{"name":"stdout","output_type":"stream","text":["Results\n","============\n","npmi: 0.0775354631383612\n","diversity: 0.698\n"," \n","ROUND 2\n"]},{"name":"stderr","output_type":"stream","text":["2024-07-25 17:53:53,622 - BERTopic - Reduced dimensionality with UMAP\n","2024-07-25 17:53:57,039 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n","2024-07-25 17:54:04,833 - BERTopic - Reduced number of topics from 385 to 11\n"]},{"name":"stdout","output_type":"stream","text":["Results\n","============\n","npmi: 0.053817398325053244\n","diversity: 0.63\n"," \n"]},{"name":"stderr","output_type":"stream","text":["2024-07-25 17:54:48,239 - BERTopic - Reduced dimensionality with UMAP\n","2024-07-25 17:54:51,489 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n","2024-07-25 17:54:59,068 - BERTopic - Reduced number of topics from 378 to 21\n"]},{"name":"stdout","output_type":"stream","text":["Results\n","============\n","npmi: 0.07189070314258413\n","diversity: 0.63\n"," \n"]},{"name":"stderr","output_type":"stream","text":["2024-07-25 17:55:44,918 - BERTopic - Reduced dimensionality with UMAP\n","2024-07-25 17:55:48,379 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n","2024-07-25 17:55:55,859 - BERTopic - Reduced number of topics from 375 to 31\n"]},{"name":"stdout","output_type":"stream","text":["Results\n","============\n","npmi: 0.06464655411271082\n","diversity: 0.66\n"," \n"]},{"name":"stderr","output_type":"stream","text":["2024-07-25 17:56:39,553 - BERTopic - Reduced dimensionality with UMAP\n","2024-07-25 17:56:42,819 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n","2024-07-25 17:56:50,076 - BERTopic - Reduced number of topics from 361 to 41\n"]},{"name":"stdout","output_type":"stream","text":["Results\n","============\n","npmi: 0.07802512522127995\n","diversity: 0.6625\n"," \n"]},{"name":"stderr","output_type":"stream","text":["2024-07-25 17:57:35,376 - BERTopic - Reduced dimensionality with UMAP\n","2024-07-25 17:57:38,763 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n","2024-07-25 17:57:46,109 - BERTopic - Reduced number of topics from 375 to 51\n"]},{"name":"stdout","output_type":"stream","text":["Results\n","============\n","npmi: 0.06742540173383561\n","diversity: 0.69\n"," \n"]}],"source":["for i in range(3):\n","    print(\"ROUND\", i)\n","    params = {\n","        \"embedding_model\": \"all-mpnet-base-v2\",\n","        \"nr_topics\": [(i+1)*10 for i in range(5)],\n","        \"min_topic_size\": 15,\n","        \"diversity\": None,\n","        \"verbose\": True\n","    }\n","\n","    trainer = Trainer(dataset=dataset,\n","                      model_name=\"BERTopic\",\n","                      params=params,\n","                      bt_embeddings=embeddings,\n","                      custom_dataset=custom,\n","                      verbose=True)\n","    results = trainer.train(save=f\"BERTopic_trump_{i+1}\")"]},{"cell_type":"markdown","metadata":{},"source":["#### Trump - BERTopic (CPU): `all-MiniLM-L6-v2`"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%capture\n","from sentence_transformers import SentenceTransformer\n","\n","# Prepare data\n","dataset, custom = \"trump\", True\n","data_loader = DataLoader(dataset)\n","_, timestamps = data_loader.load_docs()\n","data = data_loader.load_octis(custom)\n","data = [\" \".join(words) for words in data.get_corpus()]\n","\n","# Extract embeddings\n","model = SentenceTransformer(\"all-MiniLM-L6-v2\") # all-MiniLM-L6-v2, all-mpnet-base-v2, universal-sentence-encoder\n","embeddings = model.encode(data, show_progress_bar=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%time\n","MODEL_NAME = \"BERTopic_mini\"\n","for i in range(3):\n","    print(\"ROUND\", i)\n","    params = {\n","        \"embedding_model\": \"all-MiniLM-L6-v2\",\n","        \"nr_topics\": [(i+1)*10 for i in range(5)],\n","        \"min_topic_size\": 15,\n","        \"diversity\": None,\n","        \"verbose\": True\n","    }\n","\n","    trainer = Trainer(dataset=dataset,\n","                      model_name=MODEL_NAME,\n","                      params=params,\n","                      bt_embeddings=embeddings,\n","                      custom_dataset=custom,\n","                      verbose=True)\n","    results = trainer.train(save=f\"BERTopic_MiniLM_trump_{i+1}\")"]},{"cell_type":"markdown","metadata":{},"source":["#### Trump - BERTopic (CPU): `universal-sentence-encoder` (USE)\n","\n","We modified the `Trainer` class to accommodate the changes required for training with the embeddings `universal-sentence-encoder`"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class TrainerUSE:\n","\n","    def __init__(\n","        self,\n","        dataset: str,\n","        model_name: str,\n","        params: Mapping[str, Any],\n","        topk: int = 10,\n","        custom_dataset: bool = False,\n","        bt_embeddings: np.ndarray = None,\n","        bt_timestamps: List[str] = None,\n","        bt_nr_bins: int = None,\n","        custom_model=None,\n","        verbose: bool = True,\n","    ):\n","        self.dataset = dataset\n","        self.custom_dataset = custom_dataset\n","        self.model_name = model_name\n","        self.params = params\n","        self.topk = topk\n","        self.timestamps = bt_timestamps\n","        self.nr_bins = bt_nr_bins\n","        self.embeddings = bt_embeddings\n","        self.ctm_preprocessed_docs = None\n","        self.custom_model = custom_model\n","        self.verbose = verbose\n","\n","        # Prepare data and metrics\n","        self.data = self.get_dataset()\n","        self.metrics = self.get_metrics()\n","\n","        # CTM\n","        self.qt_ctm = None\n","        self.training_dataset_ctm = None\n","\n","    def train(self, save: str = False) -> Mapping[str, Any]:\n","\n","        results = []\n","\n","        # Loop over all parameters\n","        params_name = list(self.params.keys())\n","        params = {\n","            param: (value if type(value) == list else [value])\n","            for param, value in self.params.items()\n","        }\n","        new_params = list(itertools.product(*params.values()))\n","        for param_combo in new_params:\n","\n","            # Train and evaluate model\n","            params_to_use = {\n","                param: value for param, value in zip(params_name, param_combo)\n","            }\n","            output, computation_time = self._train_tm_model(params_to_use)\n","            scores = self.evaluate(output)\n","\n","            # Update results\n","            result = {\n","                \"Dataset\": self.dataset,\n","                \"Dataset Size\": len(self.data.get_corpus()),\n","                \"Model\": self.model_name,\n","                \"Params\": params_to_use,\n","                \"Scores\": scores,\n","                \"Computation Time\": computation_time,\n","            }\n","            \n","            result[\"Params\"][\"embedding_model\"] = \"USE\"\n","            results.append(result)\n","\n","        if save:\n","            with open(f\"{save}.json\", \"w\") as f:\n","                json.dump(results, f)\n","\n","            try:\n","                from google.colab import files\n","\n","                files.download(f\"{save}.json\")\n","            except ImportError:\n","                pass\n","\n","        return results\n","\n","    def _train_tm_model(\n","        self, params: Mapping[str, Any]\n","    ) -> Tuple[Mapping[str, Any], float]:\n","        \"\"\"Select and train the Topic Model\"\"\"\n","        # Train BERTopic\n","        if \"BERTopic\" in self.model_name:\n","            return self._train_bertopic(params)\n","\n","\n","    def _train_bertopic(\n","        self, params: Mapping[str, any]\n","    ) -> Tuple[Mapping[str, Any], float]:\n","        \"\"\"Train BERTopic model\"\"\"\n","        data = self.data.get_corpus()\n","        data = [\" \".join(words) for words in data]\n","        params[\"calculate_probabilities\"] = False\n","        \n","        \n","        if self.custom_model is not None:\n","            model = self.custom_model(**params)\n","        else:\n","            model = BERTopic(**params)\n","\n","        start = time.time()\n","        topics, _ = model.fit_transform(data, self.embeddings)\n","        \n","\n","        end = time.time()\n","        computation_time = float(end - start)\n","\n","        if not self.timestamps:\n","            all_words = [word for words in self.data.get_corpus() for word in words]\n","            bertopic_topics = [\n","                [\n","                    vals[0] if vals[0] in all_words else all_words[0]\n","                    for vals in model.get_topic(i)[:10]\n","                ]\n","                for i in range(len(set(topics)) - 1)\n","            ]\n","\n","            output_tm = {\"topics\": bertopic_topics}\n","\n","        return output_tm, computation_time\n","\n","    def evaluate(self, output_tm):\n","        \"\"\"Using metrics and output of the topic model, evaluate the topic model\"\"\"\n","        if self.timestamps:\n","            results = {str(timestamp): {} for timestamp, _ in output_tm.items()}\n","            for timestamp, topics in output_tm.items():\n","                self.metrics = self.get_metrics()\n","                for scorers, _ in self.metrics:\n","                    for scorer, name in scorers:\n","                        score = scorer.score(topics)\n","                        results[str(timestamp)][name] = float(score)\n","\n","        else:\n","            # Calculate results\n","            results = {}\n","            for scorers, _ in self.metrics:\n","                for scorer, name in scorers:\n","                    score = scorer.score(output_tm)\n","                    results[name] = float(score)\n","\n","            # Print results\n","            if self.verbose:\n","                print(\"Results\")\n","                print(\"============\")\n","                for metric, score in results.items():\n","                    print(f\"{metric}: {str(score)}\")\n","                print(\" \")\n","\n","        return results\n","\n","    def get_dataset(self):\n","        \"\"\"Get dataset from OCTIS\"\"\"\n","        data = Dataset()\n","\n","        if self.custom_dataset:\n","            data.load_custom_dataset_from_folder(self.dataset)\n","        else:\n","            data.fetch_dataset(self.dataset)\n","        return data\n","\n","    def get_metrics(self):\n","        \"\"\"Prepare evaluation measures using OCTIS\"\"\"\n","        npmi = Coherence(texts=self.data.get_corpus(), topk=self.topk, measure=\"c_npmi\")\n","        topic_diversity = TopicDiversity(topk=self.topk)\n","\n","        # Define methods\n","        coherence = [(npmi, \"npmi\")]\n","        diversity = [(topic_diversity, \"diversity\")]\n","        metrics = [(coherence, \"Coherence\"), (diversity, \"Diversity\")]\n","\n","        return metrics\n","\n","\n","def sent_to_words(sentences):\n","    for sentence in sentences:\n","        yield (gensim.utils.simple_preprocess(str(sentence), deacc=True))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%capture\n","from sentence_transformers import SentenceTransformer\n","import tensorflow_hub\n","import numpy as np\n","# Prepare data\n","dataset, custom = \"trump\", True\n","data_loader = DataLoader(dataset)\n","_, timestamps = data_loader.load_docs()\n","data = data_loader.load_octis(custom)\n","data = [\" \".join(words) for words in data.get_corpus()]\n","\n","# import tensorflow_hub\n","model  = tensorflow_hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n","embeddings = model(data)\n","embeddings = np.array(embeddings)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%%time\n","MODEL_NAME = \"BERTopic_USE\"\n","for i in range(3):\n","    print(\"ROUND\", i)\n","    params = {\n","        \"embedding_model\": model,\n","        \"nr_topics\": [(i+1)*10 for i in range(5)],\n","        \"min_topic_size\": 15,\n","        \"diversity\": None,\n","        \"verbose\": True\n","    }\n","\n","    trainer = TrainerUSE(dataset=dataset,\n","                      model_name=MODEL_NAME,\n","                      params=params,\n","                      bt_embeddings=embeddings,\n","                      custom_dataset=custom,\n","                      verbose=True)\n","    results = trainer.train(save=f\"BERTopic_USE_trump_{i+1}\")"]},{"cell_type":"markdown","metadata":{},"source":["### Data: 20NewsGroup, BBC News\n","\n","* The code for data processing and model evalations are almost exactly the same as the code for Trump data, so we skip them."]},{"cell_type":"markdown","metadata":{},"source":["## Dynamic topic modeling - BERTopic\n","\n","* We were only able to run with BERTopic\n","* Unable to run the LDA Sequence evaluation - too slow to be handled by Kaggle's 12-hour training limit"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-07-25T17:59:01.483866Z","iopub.status.busy":"2024-07-25T17:59:01.483497Z","iopub.status.idle":"2024-07-25T17:59:51.113814Z","shell.execute_reply":"2024-07-25T17:59:51.112993Z","shell.execute_reply.started":"2024-07-25T17:59:01.483837Z"},"trusted":true},"outputs":[],"source":["%%capture\n","from sentence_transformers import SentenceTransformer\n","\n","# Prepare data\n","dataset, custom = \"trump_dtm\", True\n","data_loader = DataLoader(dataset)\n","_, timestamps = data_loader.load_docs()\n","data = data_loader.load_octis(custom)\n","data = [\" \".join(words) for words in data.get_corpus()]\n","\n","# Extract embeddings\n","model = SentenceTransformer(\"all-mpnet-base-v2\")\n","embeddings = model.encode(data, show_progress_bar=True)"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-07-25T17:59:51.115782Z","iopub.status.busy":"2024-07-25T17:59:51.115473Z","iopub.status.idle":"2024-07-25T17:59:51.122029Z","shell.execute_reply":"2024-07-25T17:59:51.121148Z","shell.execute_reply.started":"2024-07-25T17:59:51.115755Z"},"trusted":true},"outputs":[{"data":{"text/plain":["['indexes.txt', 'corpus.tsv', 'vocabulary.txt', 'metadata.json']"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["# Match indices\n","import os\n","os.listdir(f\"./{dataset}\")"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-07-25T17:59:51.123611Z","iopub.status.busy":"2024-07-25T17:59:51.123254Z","iopub.status.idle":"2024-07-25T18:00:00.775417Z","shell.execute_reply":"2024-07-25T18:00:00.774439Z","shell.execute_reply.started":"2024-07-25T17:59:51.123550Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(44252, 44252)"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["with open(f\"./{dataset}/indexes.txt\") as f:\n","    indices = f.readlines()\n","    \n","indices = [int(index.split(\"\\n\")[0]) for index in indices]\n","timestamps = [timestamp for index, timestamp in enumerate(timestamps) if index in indices]\n","len(data), len(timestamps)"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-07-25T18:00:00.777997Z","iopub.status.busy":"2024-07-25T18:00:00.777611Z","iopub.status.idle":"2024-07-25T18:07:01.135150Z","shell.execute_reply":"2024-07-25T18:07:01.134227Z","shell.execute_reply.started":"2024-07-25T18:00:00.777962Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-07-25 18:00:35,412 - BERTopic - Reduced dimensionality with UMAP\n","2024-07-25 18:00:38,734 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n","2024-07-25 18:00:44,763 - BERTopic - Reduced number of topics from 362 to 51\n","\n","0it [00:00, ?it/s]\u001b[A\n","3it [00:00, 17.27it/s]\u001b[A\n","5it [00:00,  8.53it/s]\u001b[A\n","7it [00:00,  8.18it/s]\u001b[A\n","8it [00:00,  8.49it/s]\u001b[A\n","9it [00:01,  7.92it/s]\u001b[A\n","10it [00:01,  7.65it/s]\u001b[A\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","allstar\n","\n","\n","\n","\n","successand\n","thanksand\n","alltime\n","newshere\n","hearand\n","mailin\n","arod\n","vegaswill\n","interestingsee\n","hairshould\n","pressthey\n","fashionnot\n","man1000\n","erickthanksand\n","arod\n","allstar\n","thanksa\n","allstar\n","thanksand\n","knowwonderful\n","livetweeting\n","downnobody\n","thinksit\n","wordsgreatly\n","jobyou\n","\n","\n","\n","\n","presmake\n","transpacific\n","callins\n","importantyou\n","todaya\n","greatgood\n","\n","officewe\n","obamaruined\n","gooddeal\n","businessvery\n","antisecond\n","cnnbeing\n","teamgo\n","importantbut\n","relationshipbut\n","alltime\n","tshirt\n","\n","\n","antitrump\n","\n","\n"]},{"name":"stderr","output_type":"stream","text":["2024-07-25 18:02:58,097 - BERTopic - Reduced dimensionality with UMAP\n","2024-07-25 18:03:01,397 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n","2024-07-25 18:03:07,257 - BERTopic - Reduced number of topics from 370 to 51\n","\n","0it [00:00, ?it/s]\u001b[A\n","3it [00:00, 18.36it/s]\u001b[A\n","5it [00:00,  9.01it/s]\u001b[A\n","7it [00:00,  8.63it/s]\u001b[A\n","9it [00:00,  8.51it/s]\u001b[A\n","10it [00:01,  8.08it/s]\u001b[A\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","allstar\n","\n","\n","\n","\n","thanksand\n","unscrew\n","christmashowever\n","happenshould\n","yesit\n","rosieget\n","hearand\n","mailin\n","danafarber\n","cyberespionage\n","pressthey\n","hairshould\n","man1000\n","thanksa\n","allstar\n","allstar\n","thanksand\n","commanderinchief\n","92yearold\n","dani\n","thanksboth\n","livetweeting\n","\n","\n","\n","\n","wellearned\n","biasfree\n","selfimposed\n","thinksit\n","wordsgreatly\n","jobyou\n","transpacific\n","\n","\n","\n","\n","fivestar\n","winthx\n","awarda\n","gooddeal\n","relationshipbut\n","\n","\n","\n","\n","tshirt\n","secretarygeneral\n","trumpand\n","\n","\n"]},{"name":"stderr","output_type":"stream","text":["2024-07-25 18:05:15,782 - BERTopic - Reduced dimensionality with UMAP\n","2024-07-25 18:05:19,132 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n","2024-07-25 18:05:24,931 - BERTopic - Reduced number of topics from 364 to 51\n","\n","0it [00:00, ?it/s]\u001b[A\n","3it [00:00, 17.19it/s]\u001b[A\n","5it [00:00,  8.72it/s]\u001b[A\n","7it [00:00,  7.96it/s]\u001b[A\n","8it [00:00,  8.29it/s]\u001b[A\n","9it [00:01,  7.51it/s]\u001b[A\n","10it [00:01,  7.49it/s]\u001b[A\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","allstar\n","\n","\n","\n","\n","successand\n","thanksand\n","bowsand\n","hearand\n","builderi\n","arod\n","mailin\n","intrump\n","leadin\n","greatyou\n","pressthey\n","hairshould\n","man1000\n","thanksa\n","2554\n","arod\n","thankswork\n","allstar\n","thanksand\n","92yearold\n","pennsylvaniathus\n","happengreat\n","dani\n","wordsgreatly\n","downnobody\n","thinksit\n","ratingsand\n","\n","\n","\n","\n","transpacific\n","\n","obamaruined\n","antisecond\n","gooddeal\n","cnnbeing\n","teamgo\n","2yrsan\n","tshirt\n","mehe\n"]}],"source":["for i in range(3):\n","    params = {\n","        \"nr_topics\": [50],\n","        \"min_topic_size\": 15,\n","        \"verbose\": True,\n","    }\n","\n","    trainer = Trainer(dataset=dataset,\n","                      model_name=\"BERTopic\",\n","                      params=params,\n","                      bt_embeddings=embeddings,\n","                      custom_dataset=custom,\n","                      bt_timestamps=timestamps,\n","                      topk=5,\n","                      bt_nr_bins=10,\n","                      verbose=True)\n","    results = trainer.train(f\"DynamicBERTopic_trump_{i}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Wall time\n","\n","* We did not do this part as packages/libraries have changed significantly in the last two years, so the results won't be comparable with the original paper."]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30747,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
